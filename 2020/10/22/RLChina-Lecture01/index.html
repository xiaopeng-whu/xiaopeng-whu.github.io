<!DOCTYPE html><html lang="zh-Hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="RLChina Lecture01笔记"><meta name="keywords" content=""><meta name="author" content="Zepeng Wang"><meta name="copyright" content="Zepeng Wang"><title>RLChina Lecture01笔记 | 王小鹏's Blog</title><link rel="shortcut icon" href="/melody-favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '3.8.0'
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction-to-Reinforcement-Learning"><span class="toc-number">1.</span> <span class="toc-text">Introduction to Reinforcement Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-About-RL"><span class="toc-number">1.1.</span> <span class="toc-text">1.About RL</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Characteristics"><span class="toc-number">1.1.1.</span> <span class="toc-text">Characteristics:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Problem："><span class="toc-number">1.2.</span> <span class="toc-text">2.Problem：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Inside-An-RL-Agent："><span class="toc-number">1.3.</span> <span class="toc-text">3.Inside An RL Agent：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Categorizing-RL-Agents："><span class="toc-number">1.3.1.</span> <span class="toc-text">Categorizing RL Agents：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Markov-Decision-Processes"><span class="toc-number">1.4.</span> <span class="toc-text">4.Markov Decision Processes</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy"><span class="toc-number">1.4.1.</span> <span class="toc-text">Policy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-Function"><span class="toc-number">1.4.2.</span> <span class="toc-text">Value Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimal-Value-Function"><span class="toc-number">1.4.3.</span> <span class="toc-text">Optimal Value Function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Optimal-Policy"><span class="toc-number">1.4.4.</span> <span class="toc-text">Optimal Policy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Value-based-Methods"><span class="toc-number">2.</span> <span class="toc-text">Value-based Methods</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Dynamic-Programming"><span class="toc-number">2.1.</span> <span class="toc-text">1.Dynamic Programming</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-Evalution"><span class="toc-number">2.1.1.</span> <span class="toc-text">Policy Evalution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-Evaluation-in-Gridworld"><span class="toc-number">2.1.2.</span> <span class="toc-text">Policy Evaluation in Gridworld</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-Improvement"><span class="toc-number">2.1.3.</span> <span class="toc-text">Policy Improvement</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Policy-Iteration"><span class="toc-number">2.1.4.</span> <span class="toc-text">Policy Iteration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Value-Iteration"><span class="toc-number">2.1.5.</span> <span class="toc-text">Value Iteration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Generalized-Policy-Iteration"><span class="toc-number">2.1.6.</span> <span class="toc-text">Generalized Policy Iteration</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contraction-Mapping"><span class="toc-number">2.1.7.</span> <span class="toc-text">Contraction Mapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DP-Summary"><span class="toc-number">2.1.8.</span> <span class="toc-text">DP Summary</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Monte-Carlo"><span class="toc-number">2.2.</span> <span class="toc-text">2.Monte Carlo</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-TD-Learning"><span class="toc-number">2.3.</span> <span class="toc-text">3.TD Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Off-policy-Learning"><span class="toc-number">2.4.</span> <span class="toc-text">4.Off-policy Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-Learning"><span class="toc-number">2.4.1.</span> <span class="toc-text">Q Learning</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-DQN-and-its-variants"><span class="toc-number">2.5.</span> <span class="toc-text">5.DQN and its variants</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="https://avatars2.githubusercontent.com/u/45620521?s=400&amp;amp;u=4f8be20dbb357189266fe2457e2f4d877b0b391b&amp;amp;v=4"></div><div class="author-info__name text-center">Zepeng Wang</div><div class="author-info__description text-center"></div><div class="follow-button"><a href="https://github.com/xiaopeng-whu">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">22</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">24</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">8</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">王小鹏's Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title">RLChina Lecture01笔记</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2020-10-22</time><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">979</span><span class="post-meta__separator">|</span><span>阅读时长: 3 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><hr>
<p>老师给了强化学习的方向，发了个网课看，把第一讲的笔记整理了一下，然后确信了这个方向属实很难…<br><a id="more"></a></p>
<h1 id="Introduction-to-Reinforcement-Learning"><a href="#Introduction-to-Reinforcement-Learning" class="headerlink" title="Introduction to Reinforcement Learning"></a>Introduction to Reinforcement Learning</h1><h2 id="1-About-RL"><a href="#1-About-RL" class="headerlink" title="1.About RL"></a>1.About RL</h2><p>多学科交叉领域</p>
<p>与监督学习、无监督学习一样是机器学习的一个分支</p>
<h3 id="Characteristics"><a href="#Characteristics" class="headerlink" title="Characteristics:"></a>Characteristics:</h3><p>没有supervisor signal，只有reword signal，agent通过试错的方法</p>
<p>与环境交互</p>
<p>反馈有时是delay的</p>
<p>处理的sequential的数据，与时间相关</p>
<p>agent动作影响收到的数据</p>
<h2 id="2-Problem："><a href="#2-Problem：" class="headerlink" title="2.Problem："></a>2.Problem：</h2><p>给定agent（智能体）一个state（状态），agent作出action（动作），作用在环境上，会返回reward转移到下一个状态，the agent is to maximize the cumulative reward（最大化累积奖励回报）<br><img src="/2020/10/22/RLChina-Lecture01/1.png" alt=""><br>能够用最大化期望的累积回报来描述，这样的问题都可以用RL来解决。</p>
<h2 id="3-Inside-An-RL-Agent："><a href="#3-Inside-An-RL-Agent：" class="headerlink" title="3.Inside An RL Agent："></a>3.Inside An RL Agent：</h2><p>Policy：根据s输出动作a</p>
<p>Value function：衡量状态的好坏</p>
<p>Model：环境的模型（未知的，通过与环境交互学习模型）</p>
<h3 id="Categorizing-RL-Agents："><a href="#Categorizing-RL-Agents：" class="headerlink" title="Categorizing RL Agents："></a>Categorizing RL Agents：</h3><p>Value based：只学习value function</p>
<p>Policy based：只学习policy</p>
<p>Actor Critic：两者都学</p>
<p>Model free：不学习model（上面三种都是）</p>
<p>Model based：学习model</p>
<h2 id="4-Markov-Decision-Processes"><a href="#4-Markov-Decision-Processes" class="headerlink" title="4.Markov Decision Processes"></a>4.Markov Decision Processes</h2><p>描述了一个RL的学习环境，环境是全观察的（环境完全已知）</p>
<p>几乎所有的RL问题都可以转换成MDP</p>
<p>MDP中所有的states都有“Markov”性质（MDP的策略完全取决于当前状态）</p>
<p><a href="https://blog.csdn.net/bitcarmanlee/article/details/82819860" target="_blank" rel="noopener">马尔科夫链详解</a></p>
<p>一个MDP就是一个五元组&lt;S,A,T,R,γ&gt;</p>
<h3 id="Policy"><a href="#Policy" class="headerlink" title="Policy"></a>Policy</h3><p>一个policy π是给定状态采取动作的概率分布</p>
<h3 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h3><p>state-value function </p>
<pre><code>vπ(s)：从state s开始，按照policy π，得到一个期望的reword（E）
</code></pre><p>action-value function </p>
<pre><code>qπ(s,a)：从状态s开始，选取动作a，然后按照policy π，得到一个期望的reward（E）
</code></pre><h3 id="Optimal-Value-Function"><a href="#Optimal-Value-Function" class="headerlink" title="Optimal Value Function"></a>Optimal Value Function</h3><p>the optimal state-value function</p>
<p>the optimal action-value function</p>
<h3 id="Optimal-Policy"><a href="#Optimal-Policy" class="headerlink" title="Optimal Policy"></a>Optimal Policy</h3><p>if vπ(s)&gt;=vπ’(s), define π &gt;= π’</p>
<p>所有的optimal policy不一定只有一个，但optimal value function都是相同的</p>
<p>如果知道the optimal action-value function，则可以得到optimal policy</p>
<h1 id="Value-based-Methods"><a href="#Value-based-Methods" class="headerlink" title="Value-based Methods"></a>Value-based Methods</h1><h2 id="1-Dynamic-Programming"><a href="#1-Dynamic-Programming" class="headerlink" title="1.Dynamic Programming"></a>1.Dynamic Programming</h2><p><a href="https://blog.csdn.net/LagrangeSK/article/details/81081106" target="_blank" rel="noopener">DP</a></p>
<p>DP 知道MDP的全部知识，planning in an MDP</p>
<p>For prediction:在已知模型的基础上判断一个策略的价值函数，并在此基础上寻找到最优的策略和最优价值函数</p>
<p>For control:已知模型，直接寻找最优策略和最优价值函数</p>
<h3 id="Policy-Evalution"><a href="#Policy-Evalution" class="headerlink" title="Policy Evalution"></a>Policy Evalution</h3><p>problem：计算给定策略π的value function</p>
<p>solution：iteration of Bellman Expectation backup（反向迭代应用Bellman期望方程）</p>
<h3 id="Policy-Evaluation-in-Gridworld"><a href="#Policy-Evaluation-in-Gridworld" class="headerlink" title="Policy Evaluation in Gridworld"></a>Policy Evaluation in Gridworld</h3><p><img src="/2020/10/22/RLChina-Lecture01/2.jpg" alt=""><br>迭代计算公式没搞明白 </p>
<p> <a href="https://zhuanlan.zhihu.com/p/35559920" target="_blank" rel="noopener">动态规划寻找最优策略之policy evaluation(策略估计)</a></p>
<h3 id="Policy-Improvement"><a href="#Policy-Improvement" class="headerlink" title="Policy Improvement"></a>Policy Improvement</h3><p>Policy improvement的作用是对当前策略 π 进行提升，先讨论一个简单情况下的策略提升，再讨论全局意义上的策略提升。</p>
<h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><h3 id="Generalized-Policy-Iteration"><a href="#Generalized-Policy-Iteration" class="headerlink" title="Generalized Policy Iteration"></a>Generalized Policy Iteration</h3><h3 id="Contraction-Mapping"><a href="#Contraction-Mapping" class="headerlink" title="Contraction Mapping"></a>Contraction Mapping</h3><p>听到这里已经懵逼了，完全是念ppt（或者说翻译ppt…），感觉根本不是零基础入门的课..<br>……</p>
<h3 id="DP-Summary"><a href="#DP-Summary" class="headerlink" title="DP Summary"></a>DP Summary</h3><p>每轮迭代复杂度O(mn^2)，m actions、n states<br>维度爆炸的问题</p>
<h2 id="2-Monte-Carlo"><a href="#2-Monte-Carlo" class="headerlink" title="2.Monte Carlo"></a>2.Monte Carlo</h2><p>蒙特卡洛强化学习指：在不清楚MDP状态转移及即时奖励的情况下，直接从经历完整的Episode来学习状态价值，通常情况下某状态的价值等于在多个Episode中以该状态算得到的所有收获的平均。</p>
<p>完整的Episode 指必须从某一个状态开始，Agent与Environment交互直到终止状态，环境给出终止状态的即时收获为止。<br>……</p>
<h2 id="3-TD-Learning"><a href="#3-TD-Learning" class="headerlink" title="3.TD Learning"></a>3.TD Learning</h2><p>时序差分学习简称TD（Temporal-Difference Learning）学习，它的特点如下：和蒙特卡洛学习一样，它也从Episode学习，不需要了解模型本身；但是它可以学习不完整的Episode，通过自身的引导（bootstrapping），猜测Episode的结果，同时持续更新这个猜测。</p>
<h2 id="4-Off-policy-Learning"><a href="#4-Off-policy-Learning" class="headerlink" title="4.Off-policy Learning"></a>4.Off-policy Learning</h2><p><img src="/2020/10/22/RLChina-Lecture01/3.png" alt=""><br><img src="/2020/10/22/RLChina-Lecture01/4.png" alt=""></p>
<h3 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q Learning"></a>Q Learning</h3><h2 id="5-DQN-and-its-variants"><a href="#5-DQN-and-its-variants" class="headerlink" title="5.DQN and its variants"></a>5.DQN and its variants</h2><p>Deep Q Network</p>
<p><a href="https://zhuanlan.zhihu.com/reinforce" target="_blank" rel="noopener">辅助学习链接</a></p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Zepeng Wang</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/10/22/RLChina-Lecture01/">http://yoursite.com/2020/10/22/RLChina-Lecture01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">王小鹏's Blog</a>！</span></div></div><div class="post-meta__tag-list"></div><nav id="pagination"><div class="next-post pull-right"><a href="/2020/10/22/保研历程/"><span>保研历程（大学三年回顾）</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By Zepeng Wang</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><script src="/live2dw/lib/L2Dwidget.min.js?0c58a1486de42ac6cc1c59c7d98ae887"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true},"log":false,"tagMode":false});</script></body></html>